# BatchNorm

## 原因

机器学习领域假设训练数据和测试数据满足独立同分布的条件，但是由于训练过程中，各层参数不断变化，会使隐层的输入数据的分布变化，即"Internal Covariate Shift"。

BatchNorm的作用就是在深度神经网络训练过程中使得每一层神经网络的输入保持相同的分布。

经过BN后，目前大部分Activation的值落入非线性函数的线性区内，其对应的导数远离导数饱和区，这样来加速训练收敛过程。

如果都通过BN，那么不就跟把非线性函数替换成线性函数效果相同了？我们知道，如果是多层的线性函数变换其实这个深层是没有意义的，因为多层线性网络跟一层线性网络是等价的。这意味着网络的表达能力下降了，这也意味着深度的意义就没有了。所以BN为了保证非线性的获得，对变换后的满足均值为0方差为1的x又进行了scale加上shift操作(y=scale*x+shift)

> https://www.cnblogs.com/hoojjack/p/12350707.html